{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Implement RoBERTa on My TwitterData Set.","metadata":{"id":"JYl3kiquUWvY"}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\nimport numpy as np\nfrom scipy.special import softmax\nimport pandas as pd\nimport torch\n\n# Load the pre-trained model and tokenizer\nMODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\n\n# Function to preprocess text\ndef preprocess(text):\n    if isinstance(text, float):\n        text = str(text)\n    new_text = []\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\n# Load your dataset\ndf = pd.read_csv('/content/sample_data/company_tweet_new.csv', encoding='latin-1')\n\n# Apply the preprocess function to the 'body' column\ndf['preprocessed_body'] = df['body'].apply(preprocess)\ntexts = df['preprocessed_body'].tolist()\n\n# Define batch size\nbatch_size = 16  # Adjust based on available memory\n\n# Tokenize the data in batches\ndef tokenize_batch(texts):\n    return tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n\n# Create batches of texts\ntext_batches = [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]\n\n# Ensure the model is in evaluation mode\nmodel.eval()\n\n# Function to get sentiment scores\ndef get_sentiment_scores(text_batch):\n    inputs = tokenize_batch(text_batch)\n    with torch.no_grad():\n        outputs = model(**inputs)\n        scores = outputs.logits.detach().numpy()\n        return scores\n\n# Process each batch and store results\nall_scores = []\nfor text_batch in text_batches:\n    batch_scores = get_sentiment_scores(text_batch)\n    all_scores.extend(batch_scores)\n\n# Convert scores to probabilities\nprobs = softmax(np.array(all_scores), axis=1)\n\n# Get the predicted sentiment labels and polarity scores\npredicted_labels = np.argmax(probs, axis=1)\npolarity_scores = np.max(probs, axis=1)\n\n# Define the mapping from numeric labels to sentiment labels\nlabel_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\npredicted_sentiments = [label_map[pred] for pred in predicted_labels]\n\n# Add the results to the dataframe\ndf['sentiment'] = predicted_sentiments\ndf['polarity_score'] = polarity_scores\n\n# Save the results to a new CSV file\ndf.to_csv('/content/sample_data/twitter_data_with_sentiments.csv', index=False)\n\nprint(\"Sentiment analysis complete. Results saved to 'twitter_data_with_sentiments.csv'.\")\n","metadata":{"id":"vRa2p25WUR8o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/content/sample_data/twitter_data_with_sentiments.csv', encoding='latin-1')\n\n# Perform one-hot encoding on the 'sentiment' column\none_hot = pd.get_dummies(df['sentiment'])\n\n# Concatenate the one-hot encoded columns with the original dataframe\ndf = pd.concat([df, one_hot], axis=1)\n\n# Drop the original 'sentiment' column if desired\ndf = df.drop('sentiment', axis=1)\n\ndf.to_csv('/content/sample_data/updated_twitter_data_with_sentiments.csv', index=False)\n\nprint(\"One-hot encoding complete. Updated dataset saved to 'updated_twitter_data_with_sentiments.csv'.\")\n","metadata":{"id":"1sZLmXrYUbrH"},"execution_count":null,"outputs":[]}]}